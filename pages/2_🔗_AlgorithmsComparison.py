import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import os
from utils import calculate_statistics  # å¼•å…¥ç»Ÿè®¡å‡½æ•°

# Streamlit é¡µé¢é…ç½®
st.set_page_config(page_title="AlgorithmsComparison.py", page_icon="ğŸ”—")

# è®¾ç½®è®­ç»ƒæ•°æ®å’Œå›ºå®šæ•°æ®çš„æ–‡ä»¶å¤¹è·¯å¾„
convergence_data_dir = "result/ConvergenceData"
fixed_convergence_data_dir = "result/FixConvergenceData"
performance_data_dir = "result/comparison_performance"
fixed_performance_data_dir = "result/fixed_comparison_performance"
history_data_dir = "result/History_ConvergenceData"  # æ–°å¢å†å²æ•°æ®è·¯å¾„

# ç”¨æˆ·ç•Œé¢ - å·¦ä¾§æ é€‰æ‹©æ•°æ®æº
st.sidebar.title("Convergence Curve and Performance Comparison")

data_source = st.sidebar.radio("Select Data Source", ["Trained Data", "Fixed Data", "History Data"])  # å¢åŠ History Dataé€‰é¡¹

# æ ¹æ®é€‰æ‹©çš„æ•°æ®æºï¼Œè®¾ç½®æ•°æ®è·¯å¾„
if data_source == "Trained Data":
    available_datasets_dir = convergence_data_dir
    performance_data_dir = "result/comparison_performance"
elif data_source == "Fixed Data":
    available_datasets_dir = fixed_convergence_data_dir
    performance_data_dir = "result/fixed_comparison_performance"
else:
    available_datasets_dir = history_data_dir  # å†å²æ•°æ®è·¯å¾„

# è·å–ä¸åŒæ•°æ®é›†çš„æ–‡ä»¶å¤¹
available_datasets = [d for d in os.listdir(available_datasets_dir) if
                      os.path.isdir(os.path.join(available_datasets_dir, d))]

# ç¡®ä¿æœ‰æ•°æ®é›†å¯é€‰
if not available_datasets:
    st.error("No datasets found. Please check your data directories.")
else:
    # ç”¨æˆ·ç•Œé¢ - å·¦ä¾§æ é€‰æ‹©æ•°æ®é›†
    selected_dataset = st.sidebar.selectbox("Select Dataset", available_datasets)

    # å¦‚æœé€‰æ‹©å†å²æ•°æ®ï¼Œåˆ—å‡ºå¯ç”¨ç®—æ³•å†å²æ–‡ä»¶
    if data_source == "History Data":
        # è·å–æ‰€é€‰æ•°æ®é›†ä¸‹çš„æ‰€æœ‰ç®—æ³•çš„å†å² CSV æ–‡ä»¶
        dataset_dir = os.path.join(history_data_dir, selected_dataset)
        available_algorithms = [f for f in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, f))]

        # é»˜è®¤é€‰æ‹© "SA" ç®—æ³•ï¼Œå¦‚æœå­˜åœ¨
        default_algorithm = "SA" if "SA" in available_algorithms else available_algorithms[0]
        selected_algorithm = st.sidebar.selectbox("Select Algorithm", available_algorithms, index=available_algorithms.index(default_algorithm))

        algorithm_history_dir = os.path.join(dataset_dir, selected_algorithm)
        available_history_files = [f for f in os.listdir(algorithm_history_dir) if f.endswith('.csv')]

        if not available_history_files:
            st.error("No history data found for this algorithm.")
        else:
            # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ªå†å²æ•°æ®æ–‡ä»¶
            default_files = [available_history_files[0]] if available_history_files else []

            # ç”¨æˆ·ç•Œé¢ - å·¦ä¾§æ å¤šé€‰å†å²æ•°æ®æ–‡ä»¶
            selected_history_files = st.sidebar.multiselect("Select History Data Files", available_history_files,
                                                            default=default_files)

            if selected_history_files:
                st.subheader(f"Convergence History: {selected_algorithm}")

                # è¯»å–æ¯ä¸ªå†å²æ–‡ä»¶å¹¶è·å–æœ€å¤§è½®æ•°
                max_iters_in_files = []
                for history_file in selected_history_files:
                    history_file_path = os.path.join(algorithm_history_dir, history_file)
                    try:
                        history_df = pd.read_csv(history_file_path)
                        max_iters_in_files.append(history_df['Iteration'].max())
                    except Exception as e:
                        st.error(f"Error loading history data from {history_file}: {e}")

                # å¦‚æœæˆåŠŸè¯»å–äº†å†å²æ•°æ®ï¼Œæ·»åŠ æ»‘åŠ¨æ¡æ¥é€‰æ‹©æ˜¾ç¤ºçš„æœ€å¤§è½®æ•°
                if max_iters_in_files:
                    max_iterations = min(max_iters_in_files)  # é€‰æ‹©æœ€å°çš„æœ€å¤§è½®æ•°ï¼Œä»¥é¿å…è¶Šç•Œ
                    max_iter_display = st.sidebar.slider("Max Iterations to Display", min_value=1, max_value=max_iterations, value=10)

                    fig = go.Figure()

                    # æå–æ–‡ä»¶åä¸­çš„æ—¥æœŸå’Œæ—¶é—´æˆ³ä½œä¸ºå›¾ä¾‹
                    def extract_timestamp_from_filename(file_name):
                        parts = file_name.split('_')
                        if len(parts) >= 5:
                            date_part = parts[-2]  # YYYYMMDD
                            time_part = parts[-1].replace('.csv', '')  # HHMMSS
                            return f"{date_part}_{time_part}"
                        return file_name

                    # è¯»å–æ¯ä¸ªå†å²æ–‡ä»¶å¹¶ç»˜åˆ¶æ”¶æ•›æ›²çº¿
                    all_history_data = []
                    for history_file in selected_history_files:
                        history_file_path = os.path.join(algorithm_history_dir, history_file)

                        try:
                            history_df = pd.read_csv(history_file_path)
                            # åªæ˜¾ç¤ºç”¨æˆ·é€‰æ‹©çš„æœ€å¤§è½®æ•°æ•°æ®
                            truncated_df = history_df[history_df['Iteration'] <= max_iter_display]
                            all_history_data.append(truncated_df['Best Score'].values)

                            # æå–æ–‡ä»¶åæœ«å°¾çš„æ—¶é—´æˆ³ä½œä¸ºå›¾ä¾‹
                            timestamp = extract_timestamp_from_filename(history_file)

                            # ç»˜åˆ¶æ¯ä¸ªå†å²æ–‡ä»¶çš„æ”¶æ•›æ›²çº¿ï¼Œä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºå›¾ä¾‹åç§°
                            fig.add_trace(go.Scatter(x=truncated_df['Iteration'], y=truncated_df['Best Score'],
                                                     mode='lines+markers', name=f"{timestamp}"))

                        except Exception as e:
                            st.error(f"Error loading history data from {history_file}: {e}")

                    # é…ç½®å›¾è¡¨æ ·å¼
                    fig.update_layout(
                        title=f"Convergence Curves - {selected_algorithm}",
                        xaxis_title="Iterations",
                        yaxis_title="Best Score"
                    )
                    st.plotly_chart(fig)

                    # è°ƒç”¨å‡½æ•°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                    if all_history_data:
                        statistics_df = calculate_statistics(all_history_data, selected_history_files)

                        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯è¡¨æ ¼
                        st.subheader("Statistics Comparison")
                        st.dataframe(statistics_df)

    else:
        # è·å–æ‰€é€‰æ•°æ®é›†ä¸‹çš„æ‰€æœ‰ CSV æ–‡ä»¶
        dataset_dir = os.path.join(available_datasets_dir, selected_dataset)
        available_algorithms = [f for f in os.listdir(dataset_dir) if f.endswith('.csv')]


        # ä»æ–‡ä»¶åä¸­æå–ç®—æ³•åç§°
        def extract_algorithm_name(file_name):
            base_name = os.path.basename(file_name)
            if "psosa" in base_name:
                return "PSO_SA"
            elif "ga" in base_name:
                return "GA"
            elif "pso" in base_name:
                return "PSO"
            elif "sa" in base_name:
                return "SA"
            elif "aco" in base_name:
                return "ACO"
            elif "de" in base_name:
                return "DE"
            elif "coea" in base_name:
                return "CoEA"
            elif "eda" in base_name:
                return "EDA"
            elif "nsga2" in base_name:
                return "NSGA-II"
            else:
                return "unknown"


        # è·å–ç®—æ³•åç§°åˆ—è¡¨
        algorithm_names = [extract_algorithm_name(f) for f in available_algorithms]


        # åˆ›å»ºå‡½æ•°æ¥æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åŒ…å«æœŸæœ›çš„åˆ—ï¼Œå¹¶å¤„ç†é”™è¯¯
        def load_convergence_data(file_path, algorithm_name):
            try:
                df = pd.read_csv(file_path)

                # å¤„ç†ä¸åŒç®—æ³•ä½¿ç”¨çš„åˆ—å
                if 'Iteration' in df.columns:
                    iteration_column = 'Iteration'
                elif 'Generation' in df.columns:
                    iteration_column = 'Generation'
                else:
                    st.warning(
                        f"The file {os.path.basename(file_path)} does not contain the required 'Iteration' or 'Generation' columns.")
                    return None, None

                if 'Best Score' not in df.columns:
                    st.warning(
                        f"The file {os.path.basename(file_path)} does not contain the required 'Best Score' column.")
                    return None, None

                return df, iteration_column
            except Exception as e:
                st.error(f"Error loading {file_path}: {e}")
                return None, None


        # æ£€æŸ¥æ¯ä¸ªç®—æ³•çš„æ”¶æ•›æ•°æ®
        convergence_data = {}
        iteration_columns = {}  # è®°å½•æ¯ä¸ªç®—æ³•å¯¹åº”çš„è½®æ•°åˆ—å
        for file_name in available_algorithms:
            algorithm_name = extract_algorithm_name(file_name)
            file_path = os.path.join(dataset_dir, file_name)
            df, iteration_column = load_convergence_data(file_path, algorithm_name)
            if df is not None:
                convergence_data[algorithm_name] = df
                iteration_columns[algorithm_name] = iteration_column

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ”¶æ•›æ•°æ®ï¼Œæ˜¾ç¤ºé”™è¯¯
        if not convergence_data:
            st.error("No valid convergence data found for this dataset. Please check your CSV files.")
        else:
            # è·å–ä¸åŒç®—æ³•çš„æœ€å¤§è½®æ•°
            max_iterations = {algo: df[iteration_columns[algo]].max() for algo, df in convergence_data.items()}

            # ç¡®ä¿æ‰€æœ‰ç®—æ³•çš„ max_iterations ä¸­æ²¡æœ‰ None æˆ–ç©ºå€¼
            valid_max_iterations = [v for v in max_iterations.values() if v is not None and pd.notna(v)]

            if not valid_max_iterations:
                st.error("No valid iteration data found in the convergence files.")
            else:
                # ç”¨æˆ·é€‰æ‹©è¦æ¯”è¾ƒçš„ç®—æ³•ï¼Œé»˜è®¤åªé€‰ä¸­ SA
                selected_algorithms = st.sidebar.multiselect("Select Algorithms for Comparison", algorithm_names,
                                                             default=["SA"])

                # ç”¨æˆ·é€‰æ‹©æœ€å¤§æ˜¾ç¤ºè½®æ•°ï¼Œé»˜è®¤æ˜¾ç¤º 10 è½®
                if selected_algorithms:
                    max_iter_display = st.sidebar.slider("Max Iterations to Display", min_value=1,
                                                         max_value=max(valid_max_iterations), value=10)

                    st.title("Algorithms Comparison")
                    # ç»˜åˆ¶æ”¶æ•›æ›²çº¿

                    st.subheader("Convergence Curves Comparison")
                    st.write(f"For Dataset: {selected_dataset}")

                    # ä½¿ç”¨ Plotly ç»˜åˆ¶æ”¶æ•›æ›²çº¿
                    fig = go.Figure()

                    final_best_scores = {}

                    for algo in selected_algorithms:
                        iterations = convergence_data[algo][iteration_columns[algo]][:max_iter_display]
                        best_scores = convergence_data[algo]['Best Score'][:max_iter_display]

                        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„è½®æ•°æ•°æ®
                        if not best_scores.empty:
                            fig.add_trace(go.Scatter(x=iterations, y=best_scores, mode='lines+markers', name=f"{algo}"))

                            # è®°å½•æœ€ç»ˆçš„ Best Scoreï¼Œç¡®ä¿æ•°æ®éç©º
                            final_best_scores[algo] = best_scores.iloc[-1]
                        else:
                            final_best_scores[algo] = "No data"

                    fig.update_layout(
                        title="Convergence Curves",
                        xaxis_title="Iterations / Generations",
                        yaxis_title="Best Score",
                        legend_title="Algorithms"
                    )

                    # æ˜¾ç¤º Plotly å›¾è¡¨
                    st.plotly_chart(fig)

                    # æ˜¾ç¤ºæœ€ç»ˆå¯¹æ¯”çš„ Best Score
                    st.write("Final Best Scores Comparison")

                    # åˆ›å»ºä¸€ä¸ª DataFrame æ˜¾ç¤ºæœ€ç»ˆ Best Score
                    final_scores_df = pd.DataFrame({
                        'Algorithm': list(final_best_scores.keys()),
                        'Final Best Score': list(final_best_scores.values())
                    })

                    # æ˜¾ç¤º DataFrame
                    st.table(final_scores_df)












                # **Performance Comparison Section**
                st.subheader("Performance Comparison")
                performance_metrics = []

                for algo in selected_algorithms:
                    # ç‰¹æ®Šå¤„ç† PSO_SA æ–‡ä»¶å
                    algo_lower = "psosa" if algo == "PSO_SA" else algo.lower()

                    # è·å–ç®—æ³•å¯¹åº”çš„æ€§èƒ½æ•°æ®æ–‡ä»¶
                    performance_file_path = os.path.join(performance_data_dir, selected_dataset, f"comparison_performance_{algo_lower}.csv")
                    if os.path.exists(performance_file_path):
                        performance_df = pd.read_csv(performance_file_path)
                        # å°† inf å’Œ -inf è½¬æ¢ä¸º NaN
                        performance_df.replace([float('inf'), float('-inf')], pd.NA, inplace=True)

                        performance_metrics.append({
                            'Algorithm': algo,
                            'Iterations': max_iterations[algo],
                            'Best Score': performance_df['Best Score'].values[0],
                            'Worst Score': performance_df['Worst Score'].values[0],
                            'Best Improvement': performance_df['Best Improvement'].values[0],
                            'Average Improvement': performance_df['Average Improvement'].values[0],
                            'Time (s)': performance_df['Time (s)'].values[0],
                            'Convergence Rate': performance_df['Convergence Rate'].values[0],
                            'Relative Error': performance_df['Relative Error'].values[0],
                            'Convergence Speed (Stable Iterations)': performance_df['Convergence Speed (Stable Iterations)'].values[0]
                        })
                    else:
                        st.warning(f"Performance data for {algo} not found.")

                # å°†æ€§èƒ½æŒ‡æ ‡å±•ç¤ºæˆè¡¨æ ¼
                if performance_metrics:
                    performance_df = pd.DataFrame(performance_metrics)
                    st.table(performance_df)

                    # ç”¨æˆ·é€‰æ‹©å›¾è¡¨ç±»å‹
                    chart_type = st.selectbox("Select Chart Type", ["Combined (Bar + Line)", "Line Chart", "Bar Chart", "Area Chart"])

                    # ç”¨æˆ·é€‰æ‹©è¦å±•ç¤ºçš„æ€§èƒ½æŒ‡æ ‡
                    metrics_to_visualize = st.selectbox("Select Performance Metric", [
                        'Best Score', 'Worst Score', 'Best Improvement', 'Average Improvement', 'Time (s)'
                    ])

                    # æ›¿æ¢ inf å’Œ NaN æ•°æ®ä¸º 0
                    performance_df.replace([float('inf'), float('-inf'), pd.NA], 0, inplace=True)

                    # æç¤ºç”¨æˆ·å“ªäº›ç®—æ³•çš„å“ªäº›æŒ‡æ ‡ä¸º inf
                    for algo in selected_algorithms:
                        for metric in ['Best Score', 'Worst Score', 'Best Improvement', 'Average Improvement', 'Time (s)']:
                            # æ£€æŸ¥æŸ¥è¯¢ç»“æœæ˜¯å¦ä¸ºç©ºï¼Œé˜²æ­¢è¶Šç•Œè®¿é—®
                            filtered_data = performance_df.loc[performance_df['Algorithm'] == algo, metric]
                            if filtered_data.empty:
                                st.warning(f"No data available for {metric} in {algo}.")
                            elif filtered_data.values[0] == 0:
                                st.warning(f"{algo} has invalid values (like inf or NaN) for {metric}, replaced with 0 for visualization.")

                    import plotly.colors


                    # åŠ¨æ€è°ƒæ•´å®½åº¦ï¼Œæ ¹æ®ç®—æ³•æ•°é‡è®¾ç½®åˆç†çš„å®½åº¦
                    def get_bar_width(num_algorithms):
                        if num_algorithms <= 3:
                            return 0.1  # å°‘é‡ç®—æ³•æ—¶è¾ƒå®½
                        elif num_algorithms <= 6:
                            return 0.3  # ä¸­ç­‰æ•°é‡ç®—æ³•æ—¶é€‚ä¸­å®½åº¦
                        else:
                            return 0.2  # å¤§é‡ç®—æ³•æ—¶è¾ƒçª„


                    # ä½¿ç”¨ Plotly ç»˜åˆ¶æ€§èƒ½æ¯”è¾ƒå›¾è¡¨
                    if chart_type == "Bar Chart":
                        # è·å–ç®—æ³•æ•°é‡å¹¶åŠ¨æ€è®¾ç½®æŸ±ä½“å®½åº¦
                        bar_width = get_bar_width(len(selected_algorithms))
                        fig = go.Figure(go.Bar(
                            x=performance_df['Algorithm'],
                            y=performance_df[metrics_to_visualize],
                            width=[bar_width] * len(performance_df),
                            marker_color='lightblue'  # è®¾ç½®æŸ±ä½“é¢œè‰²ä¸ºæµ…è“è‰²
                        ))
                        fig.update_layout(title=f"{metrics_to_visualize} Comparison - Bar Chart")
                    elif chart_type == "Line Chart":
                        fig = px.line(
                            performance_df,
                            x="Algorithm",
                            y=metrics_to_visualize,
                            markers=True,
                            title=f"{metrics_to_visualize} Comparison - Line Chart"
                        )
                    elif chart_type == "Combined (Bar + Line)":
                        bar_width = get_bar_width(len(selected_algorithms))  # åŠ¨æ€è°ƒæ•´å®½åº¦
                        fig = go.Figure()
                        fig.add_trace(go.Bar(
                            x=performance_df['Algorithm'],
                            y=performance_df[metrics_to_visualize],
                            name='Bar',
                            width=[bar_width] * len(performance_df),
                            marker_color='lightblue'  # è®¾ç½®æŸ±ä½“é¢œè‰²ä¸ºæµ…è“è‰²
                        ))
                        fig.add_trace(go.Scatter(
                            x=performance_df['Algorithm'],
                            y=performance_df[metrics_to_visualize],
                            mode='lines+markers',
                            name='Line'
                        ))
                        fig.update_layout(title=f"{metrics_to_visualize} Comparison - Combined Chart")
                    elif chart_type == "Area Chart":
                        fig = go.Figure()
                        fig.add_trace(go.Scatter(
                            x=performance_df['Algorithm'],
                            y=performance_df[metrics_to_visualize],
                            fill='tozeroy',
                            mode='lines+markers',
                            name='Area'
                        ))
                        fig.update_layout(title=f"{metrics_to_visualize} Comparison - Area Chart")

                    # æ˜¾ç¤º Plotly å›¾è¡¨
                    st.plotly_chart(fig)




                else:
                    st.warning("No performance metrics found for the selected algorithms.")

